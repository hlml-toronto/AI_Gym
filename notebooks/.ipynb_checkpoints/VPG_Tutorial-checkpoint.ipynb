{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made symbolic link\n"
     ]
    }
   ],
   "source": [
    "%run setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithms\n",
    "\n",
    "This notebook is adapted from https://spinningup.openai.com/en/latest/ and relies on PyTorch and the OpenAI Gym.\n",
    "\n",
    "The purpose of this notebook is to provide template code and pedagogical commentary on a selection of RL algorithms. Ideally, people will be able to copy this notebook and insert their own neural network architecture into these templates without having to change the learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are used for defining the actions and states of the game \"LunarLander-v2\".\n",
    "\n",
    "Because of its simplicity, we'll use the LunarLander game for all of the following examples. The nice thing about OpenAI Gym is that all the different environments have essentially the same API, so that all that should be needed to modify this code for another environment is to change the input and output dimensions expected by your RL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 8\n",
    "ACTION_SPACE = 4\n",
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# actions\n",
    "DO_NOTHING = 0\n",
    "LEFT_ENGINE = 1\n",
    "MAIN_ENGINE = 2\n",
    "RIGHT_ENGINE = 3\n",
    "\n",
    "# state\n",
    "X_POS = 0\n",
    "Y_POS = 1\n",
    "X_SPEED = 2\n",
    "Y_SPEED = 3\n",
    "ANGLE = 4\n",
    "ANGLE_SPEED = 5\n",
    "FIRST_LEG = 6\n",
    "SECOND_LEG = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI provides pedagogical code for several RL algorithms, and has made the learning process simpler by ensuring that all algorithms follow the same basic set of steps. These are:\n",
    "\n",
    "1. Logger setup\n",
    "2. Random seed setting\n",
    "3. Environment instantiation\n",
    "4. Constructing the actor-critic PyTorch module via the actor_critic function passed to the algorithm function as an argument\n",
    "5. Instantiating the experience buffer\n",
    "6. Setting up callable loss functions that also provide diagnostics specific to the algorithm\n",
    "7. Making PyTorch optimizers\n",
    "8. Setting up model saving through the logger\n",
    "9. Setting up an update function that runs one epoch of optimization or one step of descent\n",
    "10. Running the main loop of the algorithm:\n",
    "\n",
    "    a) Run the agent in the environment\n",
    "    \n",
    "    b) Periodically update the parameters of the agent according to the main equations of the algorithm\n",
    "    \n",
    "    c) Log key performance metrics and save agent\n",
    "\n",
    "\n",
    "We'll go through each of these steps with additional commentary as we see the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Review of RL Algorithms\n",
    "![RL_Algo_Tree](img/RL_Algo_Tree.png)\n",
    "\n",
    "### The problem statement\n",
    "The general statement of an RL problem can be formulated as follows:\n",
    "\n",
    "The probability $p_\\theta$ of a play-out for a game composed of a sequence of state vectors $\\textbf{s}_t$ and agent actions $\\textbf{a}_t$ is factored into the policy vector $\\pi_\\theta(\\textbf{a}_t|\\textbf{s}_t)$ and the model $p(\\textbf{s}_{t+1}|\\textbf{s}_t, \\textbf{a}_t)$:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\textbf{s}_1,\\textbf{a}_1,...,\\textbf{s}_T,\\textbf{a}_T)=p(\\textbf{s}_1)\\Pi_{t=1}^{T}\\pi_{\\theta}(\\textbf{a}_t|\\textbf{s}_t)p(\\textbf{s}_{t+1}|\\textbf{s}_t,\\textbf{a}_t)\n",
    "$$\n",
    "\n",
    "There is additionally a reward, $r(\\textbf{s}_t,\\textbf{a}_t)$, given to the agent for each step in the game. The goal is to teach an agent to maximize this reward, i.e.\n",
    "\n",
    "$$\n",
    "max_{\\theta}E_{p_{\\theta}}[\\Sigma_{t}r(\\textbf{s}_t,\\textbf{a}_t)]\n",
    "$$\n",
    "\n",
    "In **model-free** RL we ignore the model and teach our agent to maximize the reward based purely on the current state and possibly the agent's history (past states and actions).\n",
    "\n",
    "In **model-based** RL our agent tries to learn a model which correctly predicts future rewards, so that the policy can be easily chosen to maximize the cumulative reward.\n",
    "\n",
    "**On-policy** learning means that the policy vector $\\pi$ is being updated using data (i.e., state-action pairs $(s,a)$) collected according to *the most recent version of the policy*. Conversely, **off-policy** learning is done by using data collected at any time according to any policy.\n",
    "\n",
    "### Additional formalisms\n",
    "There are several *value functions* which are commonly used to treat RL approaches. These are:\n",
    "\n",
    "1. The on-policy value function $V^\\pi(s) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s]$ which gives the expected return given a starting state $s$ and actions chosen according to the policy $\\pi$.\n",
    "\n",
    "2. The on-policy action-value function $Q^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a]$ which gives the expected return given a starting state $s$ and initial action $a$, with all future actions (but not necessarily this first action $a$) chosen according to the policy $\\pi$.\n",
    "\n",
    "3. The optimal value function $V^*(s) = max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s]$ which gives the expected return given a starting state $s$ and actions chosen according to the *optimal* policy.\n",
    "\n",
    "4. The optimal action-value function $Q^*(s,a) = max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a]$ which gives the expected return given a starting state $s$ and initial action $a$, with all future actions (but not necessarily this first action $a$) chosen according to the *optimal* policy.\n",
    "\n",
    "There are a set of relations called the Bellman Equations which essentially tell us how these value functions evolve over time:\n",
    "\n",
    "$V^\\pi(s)=E_{a\\sim\\pi,\\; s^\\prime\\sim P}[r(s,a)+\\gamma V^\\pi(s^\\prime)]$\n",
    "\n",
    "$Q^\\pi(s,a)=E_{s^\\prime\\sim P}\\big[r(s,a)+\\gamma E_{a^\\prime\\pi}[Q^\\pi(s^\\prime,a^\\prime)]\\big]$\n",
    "\n",
    "$V^*(s)=max_{a}E_{s^\\prime\\sim P}[r(s,a)+\\gamma V^*(s^\\prime)]$\n",
    "\n",
    "$Q^*(s,a)=E_{s^\\prime\\sim P}\\big[r(s,a)+\\gamma max_{a^\\prime}Q^*(s^\\prime,a^\\prime)\\big]$\n",
    "\n",
    "### Model-Free RL\n",
    "\n",
    "**Policy Optimization**: representing $\\pi_\\theta(a|s)$ explicitly and optimizing $\\theta$ either by gradient ascent directly on the performance objective, or by maximizing some local representation of the performance objective. Typically, this is done on-policy. This approach usually also requires learning an approximation of the on-policy value function $V^\\pi(s)$.\n",
    "\n",
    "Policy optimization is typically stable and sensible, because you are directly optimizing for the thing that you want.\n",
    "\n",
    "**Q-Learning**: learning an approximation for the optimal action-value function $Q^*(s,a)$. The objective function will usually be based on the Bellman equations, and optimization performed off-policy.\n",
    "\n",
    "Q-learning only indirectly optimizes for the agent performance, so it is less stable than policy optimization. However, because it can be done off-policy, it is far more efficient in data collection (and so potentially trains faster, if data collection is expensive).\n",
    "\n",
    "### Model-Based RL\n",
    "\n",
    "**Pure Planning**: does not represent a policy at all, but simply computes an optimal trajectory through the environment based on the current state and the model for the environment's evolution over some fixed time-window. At each step, a new 'optimal' trajectory is computed. Basically, this is a physics engine.\n",
    "\n",
    "**Expert Iteration**: builds on pure planning by using a planning algorithm which relies on a policy $\\pi_\\theta(a|s)$, such as Monte Carlo Tree Search, to generate candidate actions for the plan. This allows for a more efficient search through action space than pure planning.\n",
    "\n",
    "**Data Augmentation**: uses a model-free RL method but adds simulated data from a trained model to real data (or maybe even uses *only* simulated data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient\n",
    "\n",
    "Our first algorithm implementation will be the Vanilla Policy Gradient (VPG), a form of policy optimization. The theory behind this algorithm can actually be derived in just a few short lines. \n",
    "\n",
    "We aim to maximize the expected return $J(\\pi_\\theta)=E_{\\tau\\sim\\pi_\\theta}[R(\\tau)]$.\n",
    "\n",
    "We wish to perform this maximization via gradient ascent: $\\theta_{k+1} = \\theta_k + \\alpha\\nabla_\\theta J(\\pi_\\theta)|_{\\theta_k}$\n",
    "\n",
    "To find an expression for the gradient term, we use the following reasoning:\n",
    "\n",
    "$\\nabla_\\theta J(\\pi_\\theta) = \\nabla_\\theta E_{\\tau\\sim\\pi_\\theta}[R(\\tau)]$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;=\\int_\\tau \\nabla_\\theta P(\\tau|\\theta)R(\\tau)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;=\\int_\\tau P(\\tau|\\theta)\\nabla_\\theta \\text{log}P(\\tau|\\theta) R(\\tau)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;=E_{\\tau\\sim\\pi_\\theta}\\big[\\Sigma_{t=0}^T \\nabla_\\theta \\text{log}\\pi_\\theta(a_t|s_t) R(\\tau)\\big]$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\approx \\frac{1}{|D|}\\Sigma_{\\tau\\in D}\\big[\\Sigma_{t=0}^T \\nabla_\\theta \\text{log}\\pi_\\theta(a_t|s_t) R(\\tau)\\big]$\n",
    "\n",
    "where $D={\\tau_i}$ for $i\\in{1,...,N}$ and $|D|$ is the number of trajectories in $D$.\n",
    "\n",
    "\n",
    "Now we use a convenient identity, that is:\n",
    "$E_{x\\sim P_\\theta}\\big[\\nabla_\\theta \\text{log}P_\\theta(x)\\big] = 0$\n",
    "\n",
    "This also means that we can include any function that is independent of the choice of action, e.g. $E_{x\\sim P_\\theta}\\big[\\nabla_\\theta \\text{log}P_\\theta(x) b(s_t)\\big] = 0$. A common choice for a *baseline* function $b(s_t)$ is the *on-policy value function* $V^\\pi(s_t)$. \n",
    "\n",
    "Returning to our statement of the policy gradient, we can add as many terms as we want involving our baseline function because it is of the same form as our identity. One additional modification we will make is to condition the reward function in the gradient to only depend on *future* rewards, since past rewards are not relevant to judging the current choice of action $a$. This gives us a new expression for the policy gradient:\n",
    "\n",
    "$\\nabla_\\theta J(\\pi_\\theta) \\approx E_{\\tau\\sim\\pi_\\theta}\\bigg[\\Sigma_{t=0}^T \\nabla_\\theta \\text{log}\\pi_\\theta(a_t|s_t) \\big(\\Sigma_{t^\\prime}^T R(s_{t^\\prime}, a_{t^\\prime},s_{t^\\prime+1}) - V^\\pi(s_t)\\big)\\bigg]$\n",
    "\n",
    "Recall that $V^\\pi(s_t)$ is the expected total reward for starting at state $s_t$ and acting according to the policy $\\pi$. This form of the policy gradient then has an intuitive explanation: our RL agent will feel 'neutral' (i.e. zero policy gradient) when it gets the reward it expects.\n",
    "\n",
    "In practice, $V^\\pi(s_t)$ is usually trained simultaneously with the policy by minimizing the mean-square-error of a predictive neural net.\n",
    "\n",
    "### VPG Theory\n",
    "\n",
    "While the term $R(s_{t^\\prime}, a_{t^\\prime},s_{t^\\prime+1}) - V^\\pi(s_t)$ has an intuitive explanation, the vanilla policy gradient method uses a related function called the advantage function:\n",
    "\n",
    "$A^{\\pi_\\theta}(s_t, a_t) = Q^\\pi(s_t,a_t)-V^\\pi(s_t)$\n",
    "\n",
    "The advantage function $A^{\\pi}(s,a)$ corresponding to a policy $\\pi_\\theta$ describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi_\\theta$, assuming you act according to $\\pi_\\theta$ forever after. The gradient is then taken to be:\n",
    "\n",
    "$\\nabla_\\theta J(\\pi_\\theta) = E_{\\tau\\sim\\pi_\\theta}\\big[\\Sigma_{t=0}^T \\nabla_\\theta \\text{log}\\pi_\\theta(a_t|s_t) A^{\\pi_\\theta}(s_t, a_t)\\big]$\n",
    "\n",
    "### VPG Training Algorithm\n",
    "\n",
    "![VPG_pseudocode](img/VPG_pseudocode.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VPG Implementation\n",
    "\n",
    "The OpenAI implementation of VPG is located at https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/vpg\n",
    "\n",
    "What I provide here is the exact same code with additional comments and annotations.\n",
    "\n",
    "First, let's step through the 'core' code. We import a variety of packages, and define a few convenience functions. The first two of these are just methods to format shapes of tensors into expected formats for use elsewhere. The third builds a multi-layer perceptron with layer sizes specified by the input list `sizes`, layer activations specified by `activation`, and an optional final `output_activation` function which may differ from the previous layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more convenience function, this one is used for computing discounted future rewards, and provides the discount scaled to start at any future step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we get into the meat of the core functions. The `Actor` class template needs three methods. First, it needs some `_distribution` implementing $\\pi_\\theta(s_t)$. Second, it needs to be able to compute $\\text{log}P(a|\\pi)$. Note that this is the log-probability of a particular action according to $\\pi(s_t)$, which may be as simple as $\\text{log}\\frac{\\pi^{(i)}}{\\Sigma_i \\pi^{(i)}}$ where $\\pi^{(i)}$ are the elements of the vector $\\pi$. Finally, it needs a forward method, which simply applies $\\pi(s_t)$ to produce the action probability vector and returns this vector along with (optionally) the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define two types of `Actor`s which perform different functions. The `MLPCategoricalActor` implements $\\pi_\\theta(s_t)$ as a simple multi-layer perceptron with a final `Categorical` layer which selects one of $K$ possible actions according to probabilities output from the neural network. This is useful for actors where the control parameters are discrete variables, such as turning an engine thruster on or off. \n",
    "\n",
    "The `MLPGaussianActor` implements $\\pi_\\theta(s_t)$ as a multidimensional Gaussian distribution with mean determined by a multi-layer perceptron and unit variance. This is useful for actors where the control parameters are continuous variables, such as the force to exert on a robotic leg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCategoricalActor(Actor):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "\n",
    "class MLPGaussianActor(Actor):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some classes which are responsible for implementing the value function $V^\\pi(s_t)$. The `MLPCritic` class uses a multi-layer perceptron to take a state vector `obs` and output a value reflecting $V$ (the expected future reward).\n",
    "\n",
    "The `MLPActorCritic` class puts together several of the previously defined classes into a coherent solution to an RL problem. The `MLPActorCritic` class takes an OpenAI environment `observation_space` and chooses the appropriate MLP Actor class (either `MLPCategoricalActor` or `MLPGaussianActor` for discrete or continuous actors respectively). It also uses the `MLPCritic` class to define a value function. It then implements a `step` method which can be used in running a simulation. The `step` method takes a state vector `obs`, applies the actor's implementation of $\\pi_\\theta(s_t)$ to choose an action, and applies the actor's implementation of $V^\\pi(s_t)$ to get a value for the state. The action, value, and log-probability of the selected action are returned for use in training. If only the action is needed (i.e. when using the actor to run a simulation without training) then the method `act` can be called, which does `step` but only returns the action vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, observation_space, action_space, \n",
    "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        if isinstance(action_space, Box):\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)\n",
    "\n",
    "        # build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the vanilla policy gradient training algorithm itself. With the actor itself encapsulated in the previously defined classes this should be easier to follow.\n",
    "\n",
    "There are some logging and multithreading functions included in the OpenAI spinning up library.\n",
    "\n",
    "To install, follow the instructions at https://spinningup.openai.com/en/latest/user/installation.html\n",
    "\n",
    "Alternatively, you can try running the following commands (but if anything fails please see the Open AI instructions):\n",
    "\n",
    "`sudo apt-get update && sudo apt-get install libopenmpi-dev`\n",
    "\n",
    "`git clone https://github.com/openai/spinningup.git`\n",
    "\n",
    "`cd spinningup`\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "Pretty sure this code is slightly out of date and version specific, but I was able to hack out a working version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "from utils.logx import EpochLogger\n",
    "from utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to define a buffer class `VPGBuffer` which will accumulate data during a training run to form a batch of data to train on. The `store` method for this class takes the output from a timestep of the game or simulation and puts all the various data (e.g. the action chosen, the state vector, the reward, the value function estimation, etc.) into the appropriate storage buffers.\n",
    "\n",
    "The heart of the VPG algorithm is the estimation of the advantage function $A^{\\pi_\\theta}(s_t,a_t)$. When a simulation trajectory is completed (either because the game reached a termination state or because the epoch ended) then the `VPGBuffer` instance has a `finish_path` method to implement advantage function estimation. This is done using a method called *Generalized  Advantage Estimation*. The full details of this method can be read about here: https://arxiv.org/pdf/1506.02438.pdf\n",
    "However, the main idea is that we need to find an empirical estimation for $A^{\\pi_\\theta}(s_t, a_t)$. It turns out that a reasonable estimator can be obtained from \n",
    "\n",
    "$A^{\\pi_\\theta}(s_t, a_t) \\approx E_{s_{t+1}}\\big[Q^\\pi(s_t,a_t)-V^\\pi(s_t)\\big]$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;=E_{s_{t+1}}\\big[r_t + \\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)\\big] := E\\big[\\delta_t^{V^{\\pi,\\gamma}}\\big]$\n",
    "\n",
    "Consider these $k$-step estimators:\n",
    "![GAE_k_step_estimators](img/GAE_k_step_estimators.png)\n",
    "\n",
    "![GAE](img/GAE.png)\n",
    "\n",
    "The `finish_path` method implements this estimator function for the advantage, which then feeds into our VPG algorithm when computing the policy update. The `finish_path` method also computes the cumulative future reward for each time step in the trajectory, to be used as target data for training the value function estimator.\n",
    "\n",
    "Finally, the `get` method is called at the end of an epoch of training. The method performs some basic data pre-processing before feeding in the batch to the updating step of the algorithm. In particular, it returns a zero-mean and unit-variance transformed version of the advantage buffer. All the buffers are then packaged into a dictionary for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a VPG agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go (reward at current state plus\n",
    "        future rewards obtained along this trajectory during training) for\n",
    "        each state, to use as the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)  # Get mean/std in parallellized code\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to see the VPG training algorithm in action! Most of the documentation within the code is as clear as I could manage to explain, so I suggest reading through the code itself to see how the algorithm is implemented.\n",
    "\n",
    "I will highlight that the policy update step we derived from the VPG theory section (step 6 in the VPG algorithm pseudocode) is implemented in the third line of code of the `compute_loss_pi` function defined inside `vpg()`. As a reminder, our derivation told us that the policy gradient should be\n",
    "\n",
    "$\\hat{g}_k=\\frac{1}{|D_k|}\\Sigma_{\\tau\\in D_k}\\Sigma_t \\nabla_\\theta \\text{log}\\pi_{\\theta_k}(a_t|s_t)\\cdot\\hat{A}_t$\n",
    "\n",
    "where $D_k$ is the batch collected in that epoch, $\\tau$ enumerates trajectories in the batch, and $t$ enumerates time steps in the trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vpg(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(),  seed=0, \n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        logger_kwargs=dict(), save_freq=10):\n",
    "    \"\"\"\n",
    "    Vanilla Policy Gradient \n",
    "    (with GAE-Lambda for advantage estimation)\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: The constructor method for a PyTorch Module with a \n",
    "            ``step`` method, an ``act`` method, a ``pi`` module, and a ``v`` \n",
    "            module. The ``step`` method should accept a batch of observations \n",
    "            and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``a``        (batch, act_dim)  | Numpy array of actions for each \n",
    "                                           | observation.\n",
    "            ``v``        (batch,)          | Numpy array of value estimates\n",
    "                                           | for the provided observations.\n",
    "            ``logp_a``   (batch,)          | Numpy array of log probs for the\n",
    "                                           | actions in ``a``.\n",
    "            ===========  ================  ======================================\n",
    "            The ``act`` method behaves the same as ``step`` but only returns ``a``.\n",
    "            The ``pi`` module's forward call should accept a batch of \n",
    "            observations and optionally a batch of actions, and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       N/A               | Torch Distribution object, containing\n",
    "                                           | a batch of distributions describing\n",
    "                                           | the policy for the provided observations.\n",
    "            ``logp_a``   (batch,)          | Optional (only returned if batch of\n",
    "                                           | actions is given). Tensor containing \n",
    "                                           | the log probability, according to \n",
    "                                           | the policy, of the provided actions.\n",
    "                                           | If actions not given, will contain\n",
    "                                           | ``None``.\n",
    "            ===========  ================  ======================================\n",
    "            The ``v`` module's forward call should accept a batch of observations\n",
    "            and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``v``        (batch,)          | Tensor containing the value estimates\n",
    "                                           | for the provided observations. (Critical: \n",
    "                                           | make sure to flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n",
    "            you provided to VPG.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs of interaction (equivalent to\n",
    "            number of policy updates) to perform.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        pi_lr (float): Learning rate for policy optimizer.\n",
    "        vf_lr (float): Learning rate for value function optimizer.\n",
    "        train_v_iters (int): Number of gradient descent steps to take on \n",
    "            value function per epoch.\n",
    "        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
    "            close to 1.)\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Special function to avoid certain slowdowns from PyTorch + MPI combo.\n",
    "    setup_pytorch_for_mpi()\n",
    "\n",
    "    # Set up logger and save configuration\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    # Random seed\n",
    "    seed += 10000 * proc_id()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "\n",
    "    # Sync params across processes (MPI setup)\n",
    "    sync_params(ac)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing VPG policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss (i.e. mean square error)\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac)\n",
    "\n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        # Get loss and info values before update\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data)\n",
    "        loss_pi.backward()      # built-in method for PyTorch Module\n",
    "        mpi_avg_grads(ac.pi)    # average grads across MPI processes\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            mpi_avg_grads(ac.v)    # average grads across MPI processes\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        kl, ent = pi_info['kl'], pi_info_old['ent']\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old,\n",
    "                     KL=kl, Entropy=ent,\n",
    "                     DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "                     DeltaLossV=(loss_v.item() - v_l_old))\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v, logp)\n",
    "            logger.store(VVals=v)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len  # check if trajectory has reached max length\n",
    "            terminal = d or timeout  # check if environment sent a terminal state flag\n",
    "            epoch_ended = t==local_steps_per_epoch-1  # check if enough data has been collected for the epoch\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to /mnt/c/Users/Duncan/OneDrive - University of Toronto/Documents/University/Grad Studies Year 4/HLML/AI_Gym/notebooks/data/vpg/vpg_s0/progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{\n",
      "        \"hidden_sizes\":\t[\n",
      "            64,\n",
      "            64\n",
      "        ]\n",
      "    },\n",
      "    \"actor_critic\":\t\"MLPActorCritic\",\n",
      "    \"env_fn\":\t\"<function <lambda> at 0x7f0212d96310>\",\n",
      "    \"epochs\":\t50,\n",
      "    \"exp_name\":\t\"vpg\",\n",
      "    \"gamma\":\t0.99,\n",
      "    \"lam\":\t0.97,\n",
      "    \"logger\":\t{\n",
      "        \"<utils.logx.EpochLogger object at 0x7f01e9f98310>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"vpg\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"/mnt/c/Users/Duncan/OneDrive - University of Toronto/Documents/University/Grad Studies Year 4/HLML/AI_Gym/notebooks/data/vpg/vpg_s0\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='/mnt/c/Users/Duncan/OneDrive - University of Toronto/Documents/University/Grad Studies Year 4/HLML/AI_Gym/notebooks/data/vpg/vpg_s0/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"vpg\",\n",
      "        \"output_dir\":\t\"/mnt/c/Users/Duncan/OneDrive - University of Toronto/Documents/University/Grad Studies Year 4/HLML/AI_Gym/notebooks/data/vpg/vpg_s0\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"pi_lr\":\t0.0003,\n",
      "    \"save_freq\":\t10,\n",
      "    \"seed\":\t0,\n",
      "    \"steps_per_epoch\":\t4000,\n",
      "    \"train_v_iters\":\t80,\n",
      "    \"vf_lr\":\t0.001\n",
      "}\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 4996, \t v: 4801\n",
      "\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 29 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               0 |\n",
      "|      AverageEpRet |            -178 |\n",
      "|          StdEpRet |             105 |\n",
      "|          MaxEpRet |           -18.1 |\n",
      "|          MinEpRet |            -435 |\n",
      "|             EpLen |            88.2 |\n",
      "|      AverageVVals |          -0.183 |\n",
      "|          StdVVals |          0.0744 |\n",
      "|          MaxVVals |         -0.0193 |\n",
      "|          MinVVals |          -0.435 |\n",
      "| TotalEnvInteracts |           4e+03 |\n",
      "|            LossPi |         0.00871 |\n",
      "|             LossV |        1.34e+04 |\n",
      "|       DeltaLossPi |               0 |\n",
      "|        DeltaLossV |       -1.71e+03 |\n",
      "|           Entropy |            1.38 |\n",
      "|                KL |        7.15e-10 |\n",
      "|              Time |            1.93 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = {'env': 'LunarLander-v2',\n",
    "        'hid': 64,\n",
    "        'l': 2,\n",
    "        'gamma': 0.99,\n",
    "        'seed': 0,\n",
    "        'cpu': 1,\n",
    "        'steps': 4000,\n",
    "        'epochs': 50,\n",
    "        'exp_name': 'vpg'}\n",
    "\n",
    "\n",
    "mpi_fork(args['cpu'])  # run parallel code with mpi\n",
    "\n",
    "from utils.run_utils import setup_logger_kwargs\n",
    "logger_kwargs = setup_logger_kwargs(args['exp_name'], args['seed'])\n",
    "\n",
    "vpg(lambda : gym.make(args['env']), actor_critic=MLPActorCritic,\n",
    "    ac_kwargs=dict(hidden_sizes=[args['hid']]*args['l']), gamma=args['gamma'], \n",
    "    seed=args['seed'], steps_per_epoch=args['steps'], epochs=args['epochs'],\n",
    "    logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
