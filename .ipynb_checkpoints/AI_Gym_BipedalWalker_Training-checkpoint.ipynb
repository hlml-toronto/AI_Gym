{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with OpenAI Gym\n",
    "This notebook serves as a simple working example of how to perform RL with OpenAI's AI Gym.\n",
    "\n",
    "## Environment Setup\n",
    "1. Install swig: https://www.dev2qa.com/how-to-install-swig-on-macos-linux-and-windows/\n",
    "2. Set up a python venv (optional):\n",
    "\n",
    "`pip3 install virtualenv`\n",
    "\n",
    "`python3 -m virtualenv venv`\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "3. Install required python packages:\n",
    "`pip3 install gym==0.17.2 box2d-py==2.3.8`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "The general statement of an RL problem can be formulated as follows:\n",
    "\n",
    "The probability $p_\\theta$ of a play-out for a game composed of a sequence of state vectors $\\textbf{s}_t$ and agent actions $\\textbf{a}_t$ is factored into the policy vector $\\pi_\\theta(\\textbf{a}_t|\\textbf{s}_t)$ and the model $p(\\textbf{s}_{t+1}|\\textbf{s}_t, \\textbf{a}_t)$:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\textbf{s}_1,\\textbf{a}_1,...,\\textbf{s}_T,\\textbf{a}_T)=p(\\textbf{s}_1)\\Pi_{t=1}^{T}\\pi_{\\theta}(\\textbf{a}_t|\\textbf{s}_t)p(\\textbf{s}_{t+1}|\\textbf{s}_t,\\textbf{a}_t)\n",
    "$$\n",
    "\n",
    "There is additionally a reward, $r(\\textbf{s}_t,\\textbf{a}_t)$, given to the agent for each step in the game. The goal is to teach an agent to maximize this reward, i.e.\n",
    "\n",
    "$$\n",
    "max_{\\theta}E_{p_{\\theta}}[\\Sigma_{t}r(\\textbf{s}_t,\\textbf{a}_t)]\n",
    "$$\n",
    "\n",
    "In model-free RL we ignore the model and teach our agent to maximize the reward based purely on the current state and possibly the agent's history (past states and actions).\n",
    "\n",
    "In model-based RL our agent tries to learn a model which correctly predicts future rewards, so that the policy can be easily chosen to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipedal Walker\n",
    "The following variables are used for defining the actions and states of the game \"BipedalWalker-v3\" from the OpenAI gym\n",
    "\n",
    "BipedalWalker has 2 legs. Each leg has 2 joints. You can apply the torque on each of these joints in the range of (-1, 1)\n",
    "\n",
    "The state of the game is given by 24 variables, described in more detail here: https://github.com/openai/gym/wiki/BipedalWalker-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 24\n",
    "ACTION_SPACE = 4\n",
    "ENV = gym.make(\"BipedalWalker-v3\")\n",
    "\n",
    "# actions\n",
    "Hip_1 = 0\n",
    "Knee_1 = 1\n",
    "Hip_2 = 2\n",
    "Knee_2 = 3\n",
    "\n",
    "# state\n",
    "HULL_ANGLE = 0\n",
    "HULL_ANGULAR_VELOCITY = 1\n",
    "VEL_X = 2\n",
    "VEL_Y = 3\n",
    "HIP_JOINT_1_ANGLE = 4\n",
    "HIP_JOINT_1_SPEED = 5\n",
    "KNEE_JOINT_1_ANGLE = 6\n",
    "KNEE_JOINT_1_SPEED = 7\n",
    "LEG_1_GROUND_CONTACT_FLAG = 8\n",
    "HIP_JOINT_2_ANGLE = 9\n",
    "HIP_JOINT_2_SPEED = 10\n",
    "KNEE_JOINT_2_ANGLE = 11\n",
    "KNEE_JOINT_2_SPEED = 12\n",
    "LEG_2_GROUND_CONTACT_FLAG = 13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the basic interface with the game. The game is essentially a very rapid turn-based game. In each round, there are 2 main steps:\n",
    "1. An action is taken by the agent according to the agent's policy function.\n",
    "2. The game updates according to its current state and the input action from the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    The base class for all agent-based reinforcement learning.\n",
    "    Provides default implementations for __init__() and play() methods.\n",
    "    \n",
    "    __init__() by default only requires a policy function, which selects the next action for the agent to take\n",
    "    \n",
    "    play() steps through the game using the policy function provided by the user to select an action at each step\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, policy_function):\n",
    "        self.policy = policy_function\n",
    "    \n",
    "    \n",
    "    def play(self, env, steps=500, show=False, verbose=False, sample_flag=False, *args, **kwargs):        \n",
    "        state = env.reset()\n",
    "        \n",
    "        if sample_flag:\n",
    "            state_history = []\n",
    "            action_history = []\n",
    "            reward_history = []\n",
    "            state_history.append(state)\n",
    "        cumulative_score = 0\n",
    "        if show:\n",
    "            env.render()\n",
    "        \n",
    "        # Main loop of game iteration    \n",
    "        for step in range(steps):\n",
    "            if verbose:\n",
    "                print(\"state:\", state)\n",
    "            \n",
    "            action = self.policy(state, *args, **kwargs)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"action:\", action)\n",
    "            \n",
    "            state, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            if terminal:\n",
    "                break\n",
    "            if sample_flag:\n",
    "                state_history.append(state)\n",
    "                action_history.append(action)\n",
    "                reward_history.append(reward)\n",
    "            if show:\n",
    "                env.render()\n",
    "            cumulative_score += reward\n",
    "        steps_taken = step\n",
    "        env.close()\n",
    "        \n",
    "        if sample_flag:\n",
    "            X = np.array([[state_history[s], action_history[s]] for s in range(steps_taken)])\n",
    "            Y = np.array([[state_history[s+1], reward_history[s]] for s in range(steps_taken)])\n",
    "            return X, Y\n",
    "        else:\n",
    "            return cumulative_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BipedalWalker-v3 game, the agent gets a positive reward proportional to the distance walked on the terrain. It can get a total of 300+ reward all the way up to the end.\n",
    "\n",
    "If agent tumbles, it gets a reward of -100.\n",
    "There is some negative reward proportional to the torque applied on the joint so that agent learns to walk smoothly with minimal torque.\n",
    "\n",
    "Let's define a couple of test policy functions to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"This agent returns random actions.\"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "\n",
    "\n",
    "def stupid_policy(state):\n",
    "    \"\"\"A very simple expert system.\"\"\"\n",
    "    if state[LEG_1_GROUND_CONTACT_FLAG] == 1 and state[LEG_2_GROUND_CONTACT_FLAG] == 1:\n",
    "        return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "    if state[KNEE_JOINT_1_SPEED] < -0.5:\n",
    "        return np.array([-0.05, -0.2, 0., 0.])\n",
    "    if state[KNEE_JOINT_2_SPEED] < -0.5:\n",
    "        return np.array([0., 0., -0.05, -0.2])\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.60230417908576"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent = Agent(stupid_policy)\n",
    "my_agent.play(ENV, steps=100, show=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free RL\n",
    "\n",
    "This section will provide a simple template for model-free reinforcement learning.\n",
    "\n",
    "The goal in model-free RL is to sample game iterations and thereby train a model which correctly predicts the future reward given the current state and candidate action. The 'future reward' may be the reward in the next step of the game, or the cumulative reward over many future steps.\n",
    "\n",
    "The choice of action can then be as simple as choosing the action which leads to the largest future reward.\n",
    "\n",
    "Below, we define a basic ModelFreeAgent class which can serve as a template for creating your own custom model-free agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeAgent(Agent):\n",
    "    \"\"\"\n",
    "    Template for a model-free RL agent. \n",
    "    \n",
    "    To instantiate an agent, the user must provide the following functions:\n",
    "    \n",
    "    policy_function(): A method for choosing what action to take next. This can be a selection among a  discrete set\n",
    "                       of actions (as in the probability of each legal move for a current board position),\n",
    "                       or a function which chooses an action vector by some heuristic (applied when the action space \n",
    "                       is a continuous-valued vector with like the four torques for our Bipedal Walker).\n",
    "    \n",
    "    action_generator(): A method for generating candidate actions. This can be deterministic (generate *all* legal moves\n",
    "                        for a given chess board position) or probabilistic (generate candidate actions by sampling from\n",
    "                        some - possibly learned - distribution over the action space)\n",
    "                           \n",
    "    reward_predictor(): A method for predicting the reward for a given state:action pair (s_t, a_t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_function, action_generator, reward_predictor):\n",
    "        self.policy = policy_function\n",
    "        self.action_generator = action_generator\n",
    "        self.reward_predictor = reward_predictor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_free_policy(current_state, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    The model-free agent chooses an action by generating candidate actions, predicting the future reward for each\n",
    "    candidate action, and then applying its policy function to select among the action:reward pairs. In this example\n",
    "    the policy is simply to choose the action which maximizes the predicted reward in the next time step.\n",
    "\n",
    "    Returns the next action to be input to the environment.\n",
    "    \"\"\"\n",
    "    action_generator = kwargs['action_generator']\n",
    "    reward_predictor = kwargs['reward_predictor']\n",
    "    \n",
    "    # Generate a list of candidate actions\n",
    "    candidate_actions =  action_generator(current_state)\n",
    "    # Predict the future reward for each candidate action\n",
    "    action_reward_pairs = []\n",
    "    for action in candidate_actions:\n",
    "        reward = reward_predictor(current_state, action)\n",
    "        action_reward_pairs.append((action, reward))\n",
    "    # Apply the policy function to the list of action:reward pairs. \n",
    "    # In this case, the policy is to choose the maximum predicted reward for the next time step.\n",
    "    best_action = max(action_reward_pairs, key=itemgetter(1))[0]\n",
    "\n",
    "    return best_action\n",
    "\n",
    "        \n",
    "def model_free_action_generator(state):\n",
    "    \"\"\"\n",
    "    The action generator for our model-free agent. In this example, the function generates four random action vectors.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=(4,4))\n",
    "\n",
    "\n",
    "def model_free_reward_predictor(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The function to predict the reward for a given environment state and candidate action.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-100.0, high=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our template model-free agent. We expect it to do about as well as the random agent from above, since the candidate actions and predicted rewards are providing absolutely zero additional information to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.1042864389407208"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_free_agent = ModelFreeAgent(model_free_policy, model_free_action_generator, model_free_reward_predictor)\n",
    "\n",
    "my_model_free_agent.play(ENV,\n",
    "                         steps=100,\n",
    "                         action_generator=my_model_free_agent.action_generator,\n",
    "                         reward_predictor=my_model_free_agent.reward_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based RL\n",
    "\n",
    "This section will provide a simple template for model-based reinforcement learning.\n",
    "\n",
    "The goal in model-based RL is to sample game iterations and thereby train a model which correctly predicts the future environment state and corresponding reward, given the current state and candidate action. \n",
    "\n",
    "The main advantage of model-based RL is that a model for the environment's evolution allows the agent to predict the cumulative reward over many future steps. This allows the agent to 'plan ahead', hopefully leading to a more successful policy.\n",
    "\n",
    "The choice of action can then be as simple as choosing the action which leads to the largest cumulative future reward.\n",
    "\n",
    "Below, we define a basic ModelBasedAgent class which can serve as a template for creating your own custom model-based agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent(Agent):\n",
    "    \"\"\"\n",
    "    Template for a model-based RL agent. \n",
    "    \n",
    "    To instantiate an agent, the user must provide the following functions:\n",
    "    \n",
    "    policy_function(): A method for choosing what action to take next. This can be a selection among a  discrete set\n",
    "                       of actions (as in the probability of each legal move for a current board position),\n",
    "                       or a function which chooses an action vector by some heuristic (applied when the action space \n",
    "                       is a continuous-valued vector with like the four torques for our Bipedal Walker).\n",
    "                       \n",
    " \n",
    "    \n",
    "    action_generator(): A method for generating candidate actions. This can be deterministic (generate *all* legal moves\n",
    "                        for a given chess board position) or probabilistic (generate candidate actions by sampling from\n",
    "                        some - possibly learned - distribution over the action space)\n",
    "                        \n",
    "    environment_model(): A method for predicting the next environment state given the current environment state and the \n",
    "                         candidate action. \n",
    "                         \n",
    "    reward_predictor(): A method for predicting the reward for a given state:action pair (s_t, a_t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_function, action_generator, environment_model, reward_predictor):\n",
    "        self.policy = policy_function\n",
    "        self.action_generator = action_generator\n",
    "        self.environment_model = environment_model        \n",
    "        self.reward_predictor = reward_predictor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_based_policy(current_state, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    The model-based agent chooses an action by iteratively generating candidate actions and the corresponding\n",
    "    predicted future environment state, up to some depth, and then predicting the reward for each\n",
    "    candidate action in each trajectory. \n",
    "    The agent then applies its policy function to select the action which is expected to maximize the future\n",
    "    cumulative reward. \n",
    "    \n",
    "    In this example the policy is simply to choose the action which maximizes the predicted cumulative reward\n",
    "    over the next <depth> number of steps. That is, the action which leads to the largest total reward across\n",
    "    all subsequent steps.\n",
    "    \n",
    "    This implementation generates the default number of candidate trajectories from the action_generator method\n",
    "    (say, <C> candidate trajectories) at each step, leading to <C>**<depth> random sample trajectories to compare.\n",
    "\n",
    "    Returns the next action to be input to the environment.\n",
    "    \"\"\"\n",
    "    action_generator = kwargs['action_generator']\n",
    "    environment_model = kwargs['environment_model']\n",
    "    reward_predictor = kwargs['reward_predictor']\n",
    "    depth = kwargs['depth']\n",
    "    discount = kwargs['discount']\n",
    "    \n",
    "    assert 0.0 < discount < 1.0\n",
    "        \n",
    "    # Generate a list of randomly sampled game trajectories\n",
    "    sampled_trajectories = {\"0\": [current_state, 0, 0]}\n",
    "    for d in range(depth):\n",
    "        keys_to_expand = [k for k in sampled_trajectories.keys() if k[-1]==str(d)]\n",
    "\n",
    "        for key in keys_to_expand:\n",
    "            state = sampled_trajectories[key][0]\n",
    "            candidate_actions =  action_generator(state)\n",
    "            \n",
    "            for num, action in enumerate(candidate_actions):\n",
    "                new_state = environment_model(state, action)\n",
    "                reward = reward_predictor(current_state, action)\n",
    "                new_key = key + str(num)\n",
    "                sampled_trajectories.update({new_key: [new_state, action, reward]})\n",
    "                # Now add the discounted reward from the candidate action to the parent node in the search tree\n",
    "                sampled_trajectories[key][2] += discount**d * reward\n",
    "                \n",
    "    # Apply the policy function to the sampled trajectories. \n",
    "    # In this case, the policy is to choose the maximum cumulative predicted reward at depth 1.\n",
    "    action_reward_pairs = [(el[1], el[2]) for key, el in sampled_trajectories.items() if key[-1]==\"1\"]\n",
    "    best_action = max(action_reward_pairs, key=itemgetter(1))[0]\n",
    "\n",
    "    return best_action\n",
    "\n",
    "        \n",
    "def model_based_action_generator(state):\n",
    "    \"\"\"\n",
    "    The action generator for our model-free agent. In this example, the function generates four random action vectors.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=(4,4))\n",
    "\n",
    "\n",
    "def model_based_environment_model(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The environment model for our model-based agent. In this example, the function generates a random state vector.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return ENV.observation_space.sample()\n",
    "\n",
    "\n",
    "def model_based_reward_predictor(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The function to predict the reward for a given environment state and candidate action.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-100.0, high=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our template model-based agent. We expect it to do about as well as the random agent from above, since the candidate actions and predicted rewards are providing absolutely zero additional information to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.948432776862747"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_based_agent = ModelBasedAgent(model_based_policy, model_based_action_generator,\n",
    "                                       model_based_environment_model, model_based_reward_predictor)\n",
    "\n",
    "my_model_based_agent.play(ENV,\n",
    "                          steps=100,\n",
    "                          action_generator=my_model_based_agent.action_generator,\n",
    "                          environment_model=my_model_based_agent.environment_model,\n",
    "                          reward_predictor=my_model_based_agent.reward_predictor,\n",
    "                          depth=3,\n",
    "                          discount=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RL Training\n",
    "\n",
    "Now we will build a training loop for our reward predictor and environment model. This will require us to sample many independent tuples of \n",
    "\n",
    "(state, action) -> (new_state, reward)\n",
    "\n",
    "We can do this with the Agent method play using the sample_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-142-5cad8617fbab>:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X = np.array([[state_history[s], action_history[s]] for s in range(steps_taken)])\n",
      "<ipython-input-142-5cad8617fbab>:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Y = np.array([[state_history[s+1], reward_history[s]] for s in range(steps_taken)])\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_raw = my_model_free_agent.play(ENV,\n",
    "                                steps=100,\n",
    "                                sample_flag=True,\n",
    "                                action_generator=my_model_free_agent.action_generator,\n",
    "                                reward_predictor=my_model_free_agent.reward_predictor)\n",
    "\n",
    "# For each sample, concatenate the state and action vectors into a single vector of \n",
    "# length (STATE_SPACE + ACTION_SPACE)\n",
    "x_cat = [np.concatenate((x_raw[r,0], x_raw[r,1])) for r in range(len(x_raw))]\n",
    "\n",
    "# For each sample, concatenate the new state and reward into a single vector of \n",
    "# length STATE_SPACE + 1 \n",
    "y_cat = [np.concatenate((y_raw[r,0], [y_raw[r,1]])) for r in range(len(y_raw))]\n",
    "\n",
    "# Define Pytorch tensors which will be used for training\n",
    "X = torch.tensor(x_cat, dtype=torch.float32)\n",
    "Y = torch.tensor(y_cat, dtype=torch.float32)\n",
    "Y_reward = torch.unsqueeze(Y[:,-1], 1)\n",
    "Y_new_state = Y[:,0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a basic feedforward neural network to predict the reward given a state and an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardNet(\n",
      "  (fc1): Linear(in_features=28, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (fc3): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RewardNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(STATE_SPACE + ACTION_SPACE, 16)  # 16 node feedforward layer\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the activation functions. Pytorch will automatically build the backward method.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "test_reward_predictor = RewardNet()\n",
    "print(test_reward_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I build a PyTorch dataset object, which is just to make sure that our data interfaces with various PyTorch classes nicely. I also define the loss function to be mean square error implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2.7473e-03, -2.4620e-05,  1.9152e-03, -1.6000e-02,  9.1814e-02,\n",
      "        -2.5274e-03,  8.6035e-01,  3.2701e-03,  1.0000e+00,  3.2219e-02,\n",
      "        -2.5272e-03,  8.5390e-01,  1.7874e-03,  1.0000e+00,  4.4081e-01,\n",
      "         4.4582e-01,  4.6142e-01,  4.8955e-01,  5.3410e-01,  6.0246e-01,\n",
      "         7.0915e-01,  8.8593e-01,  1.0000e+00,  1.0000e+00,  6.0929e-01,\n",
      "        -5.8757e-01, -7.0188e-01,  7.6751e-01]), tensor([-0.1024]))\n",
      "tensor(0.0186, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "reward_dataset = TensorDataset(X, Y_reward)\n",
    "sample = reward_dataset.__getitem__(0)\n",
    "print(sample)\n",
    "criterion = nn.MSELoss() # this is an instantiation of a class defined in the PyTorch nn library\n",
    "loss = criterion(test_reward_predictor(sample[0]), sample[1])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation is easily performed in PyTorch using the backprop method. First, clear the gradient buffer. This must be done because the backprop method accumulates gradients from existing gradients by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.bias.grad before backward\n",
      "None\n",
      "fc1.bias.grad after backward\n",
      "tensor([ 0.0162,  0.0183,  0.0000,  0.0018,  0.0000, -0.0061,  0.0000,  0.0215,\n",
      "         0.0000, -0.0072, -0.0063, -0.0057,  0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_reward_predictor.zero_grad()\n",
    "\n",
    "print('fc1.bias.grad before backward')\n",
    "print(test_reward_predictor.fc1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('fc1.bias.grad after backward')\n",
    "print(test_reward_predictor.fc1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine everything we've seen into a training function that we can call to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, batch_size, epochs):\n",
    "    # Select optimizing algorithm\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    \n",
    "    # Build the dataset into a DataLoader \n",
    "    dl = DataLoader(data, batch_size=batch_size)\n",
    "    \n",
    "    # training loop:\n",
    "    for epoch in range(epochs):\n",
    "        batch, target = next(iter(dl))\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(batch)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
