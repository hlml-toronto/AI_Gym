{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with OpenAI Gym\n",
    "This notebook serves as a simple working example of how to perform RL with OpenAI's AI Gym.\n",
    "\n",
    "## Environment Setup\n",
    "1. Install swig: https://www.dev2qa.com/how-to-install-swig-on-macos-linux-and-windows/\n",
    "2. Set up a python venv (optional):\n",
    "\n",
    "`pip3 install virtualenv`\n",
    "\n",
    "`python3 -m virtualenv venv`\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "3. Install required python packages:\n",
    "`pip3 install gym==0.17.2 box2d-py==2.3.8`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "The general statement of an RL problem can be formulated as follows:\n",
    "\n",
    "The probability $p_\\theta$ of a play-out for a game composed of a sequence of state vectors $\\textbf{s}_t$ and agent actions $\\textbf{a}_t$ is factored into the policy vector $\\pi_\\theta(\\textbf{a}_t|\\textbf{s}_t)$ and the model $p(\\textbf{s}_{t+1}|\\textbf{s}_t, \\textbf{a}_t)$:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\textbf{s}_1,\\textbf{a}_1,...,\\textbf{s}_T,\\textbf{a}_T)=p(\\textbf{s}_1)\\Pi_{t=1}^{T}\\pi_{\\theta}(\\textbf{a}_t|\\textbf{s}_t)p(\\textbf{s}_{t+1}|\\textbf{s}_t,\\textbf{a}_t)\n",
    "$$\n",
    "\n",
    "There is additionally a reward, $r(\\textbf{s}_t,\\textbf{a}_t)$, given to the agent for each step in the game. The goal is to teach an agent to maximize this reward, i.e.\n",
    "\n",
    "$$\n",
    "max_{\\theta}E_{p_{\\theta}}[\\Sigma_{t}r(\\textbf{s}_t,\\textbf{a}_t)]\n",
    "$$\n",
    "\n",
    "In model-free RL we ignore the model and teach our agent to maximize the reward based purely on the current state and possibly the agent's history (past states and actions).\n",
    "\n",
    "In model-based RL our agent tries to learn a model which correctly predicts future rewards, so that the policy can be easily chosen to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipedal Walker\n",
    "The following variables are used for defining the actions and states of the game \"BipedalWalker-v3\" from the OpenAI gym\n",
    "\n",
    "BipedalWalker has 2 legs. Each leg has 2 joints. You can apply the torque on each of these joints in the range of (-1, 1)\n",
    "\n",
    "The state of the game is given by 24 variables, described in more detail here: https://github.com/openai/gym/wiki/BipedalWalker-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duncan/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "STATE_SPACE = 24\n",
    "ACTION_SPACE = 4\n",
    "ENV = gym.make(\"BipedalWalker-v3\")\n",
    "\n",
    "# actions\n",
    "Hip_1 = 0\n",
    "Knee_1 = 1\n",
    "Hip_2 = 2\n",
    "Knee_2 = 3\n",
    "\n",
    "# state\n",
    "HULL_ANGLE = 0\n",
    "HULL_ANGULAR_VELOCITY = 1\n",
    "VEL_X = 2\n",
    "VEL_Y = 3\n",
    "HIP_JOINT_1_ANGLE = 4\n",
    "HIP_JOINT_1_SPEED = 5\n",
    "KNEE_JOINT_1_ANGLE = 6\n",
    "KNEE_JOINT_1_SPEED = 7\n",
    "LEG_1_GROUND_CONTACT_FLAG = 8\n",
    "HIP_JOINT_2_ANGLE = 9\n",
    "HIP_JOINT_2_SPEED = 10\n",
    "KNEE_JOINT_2_ANGLE = 11\n",
    "KNEE_JOINT_2_SPEED = 12\n",
    "LEG_2_GROUND_CONTACT_FLAG = 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2787744 ,  1.1125426 , -0.67280614, -1.0236198 ,  0.55412406,\n",
       "        1.2463422 ,  0.19889538,  0.9246162 , -1.2400373 , -0.5879139 ,\n",
       "       -1.9885919 ,  0.56692463,  1.1481102 , -1.1484573 ,  1.9298387 ,\n",
       "        0.65511125,  0.02889112, -1.6323286 ,  0.0808932 , -1.3743677 ,\n",
       "       -0.02229984,  0.55941194, -1.019841  ,  0.29258353], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the basic interface with the game. The game is essentially a very rapid turn-based game. In each round, there are 2 main steps:\n",
    "1. An action is taken by the agent according to the agent's policy function.\n",
    "2. The game updates according to its current state and the input action from the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    The base class for all agent-based reinforcement learning.\n",
    "    Provides default implementations for __init__() and play() methods.\n",
    "    \n",
    "    __init__() by default only requires a policy function, which selects the next action for the agent to take\n",
    "    \n",
    "    play() by default steps through the game using the policy function provided by the user to select an action at each step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_function):\n",
    "        self.policy = policy_function\n",
    "    \n",
    "    def play(self, env, steps=500, score_augmentation=False, show=False, verbose=False, *args, **kwargs):\n",
    "        state = env.reset()\n",
    "        cumulative_score = 0\n",
    "        if show:\n",
    "            env.render()\n",
    "        for step in range(steps):\n",
    "            if verbose:\n",
    "                print(\"state:\", state)\n",
    "            action = self.policy(state, *args, **kwargs)\n",
    "            if verbose:\n",
    "                print(\"action:\", action)\n",
    "            state, reward, terminal, info = env.step(action)\n",
    "            if show:\n",
    "                env.render()\n",
    "            cumulative_score += reward\n",
    "            if terminal:\n",
    "                break\n",
    "        env.close()\n",
    "        return cumulative_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BipedalWalker-v3 game, the agent gets a positive reward proportional to the distance walked on the terrain. It can get a total of 300+ reward all the way up to the end.\n",
    "\n",
    "If agent tumbles, it gets a reward of -100.\n",
    "There is some negative reward proportional to the torque applied on the joint so that agent learns to walk smoothly with minimal torque.\n",
    "\n",
    "Let's define a couple of test policy functions to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"This agent returns random actions.\"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "\n",
    "\n",
    "def stupid_policy(state):\n",
    "    \"\"\"A very simple expert system.\"\"\"\n",
    "    if state[LEG_1_GROUND_CONTACT_FLAG] == 1 and state[LEG_2_GROUND_CONTACT_FLAG] == 1:\n",
    "        return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "    if state[KNEE_JOINT_1_SPEED] < -0.5:\n",
    "        return np.array([-0.05, -0.2, 0., 0.])\n",
    "    if state[KNEE_JOINT_2_SPEED] < -0.5:\n",
    "        return np.array([0., 0., -0.05, -0.2])\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-97.54617795890783"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent = Agent(stupid_policy)\n",
    "my_agent.play(ENV, steps=100, show=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free RL\n",
    "\n",
    "This section will provide a simple template for model-free reinforcement learning.\n",
    "\n",
    "The goal in model-free RL is to sample game iterations and thereby train a model which correctly predicts the future reward given the current state and candidate action. The 'future reward' may be the reward in the next step of the game, or the cumulative reward over many future steps.\n",
    "\n",
    "The choice of action can then be as simple as choosing the action which leads to the largest future reward.\n",
    "\n",
    "Below, we define a basic ModelFreeAgent class which can serve as a template for creating your own custom model-free agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeAgent(Agent):\n",
    "    \"\"\"\n",
    "    Template for a model-free RL agent. \n",
    "    \n",
    "    To instantiate an agent, the user must provide the following functions:\n",
    "    \n",
    "    policy_function(): A method for choosing what action to take next. This can be a selection among a  discrete set\n",
    "                       of actions (as in the probability of each legal move for a current board position),\n",
    "                       or a function which chooses an action vector by some heuristic (applied when the action space \n",
    "                       is a continuous-valued vector with like the four torques for our Bipedal Walker).\n",
    "    \n",
    "    action_generator(): A method for generating candidate actions. This can be deterministic (generate *all* legal moves\n",
    "                        for a given chess board position) or probabilistic (generate candidate actions by sampling from\n",
    "                        some - possibly learned - distribution over the action space)\n",
    "                           \n",
    "    reward_predictor(): A method for predicting the reward for a given state:action pair (s_t, a_t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_function, action_generator, reward_predictor):\n",
    "        self.policy = policy_function\n",
    "        self.action_generator = action_generator\n",
    "        self.reward_predictor = reward_predictor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_free_policy(current_state, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    The model-free agent chooses an action by generating candidate actions, predicting the future reward for each\n",
    "    candidate action, and then applying its policy function to select among the action:reward pairs. In this example\n",
    "    the policy is simply to choose the action which maximizes the predicted reward in the next time step.\n",
    "\n",
    "    Returns the next action to be input to the environment.\n",
    "    \"\"\"\n",
    "    action_generator = kwargs['action_generator']\n",
    "    reward_predictor = kwargs['reward_predictor']\n",
    "    \n",
    "    # Generate a list of candidate actions\n",
    "    candidate_actions =  action_generator(current_state)\n",
    "    # Predict the future reward for each candidate action\n",
    "    action_reward_pairs = []\n",
    "    for action in candidate_actions:\n",
    "        reward = reward_predictor(current_state, action)\n",
    "        action_reward_pairs.append((action, reward))\n",
    "    # Apply the policy function to the list of action:reward pairs. \n",
    "    # In this case, the policy is to choose the maximum predicted reward for the next time step.\n",
    "    best_action = max(action_reward_pairs, key=itemgetter(1))[0]\n",
    "\n",
    "    return best_action\n",
    "\n",
    "        \n",
    "def model_free_action_generator(state):\n",
    "    \"\"\"\n",
    "    The action generator for our model-free agent. In this example, the function generates four random action vectors.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=(4,4))\n",
    "\n",
    "\n",
    "def model_free_reward_predictor(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The function to predict the reward for a given environment state and candidate action.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-100.0, high=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our template model-free agent. We expect it to do about as well as the random agent from above, since the candidate actions and predicted rewards are providing absolutely zero additional information to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-101.88813562640368"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_free_agent = ModelFreeAgent(model_free_policy, model_free_action_generator, model_free_reward_predictor)\n",
    "\n",
    "my_model_free_agent.play(ENV,\n",
    "                         steps=100,\n",
    "                         action_generator=my_model_free_agent.action_generator,\n",
    "                         reward_predictor=my_model_free_agent.reward_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based RL\n",
    "\n",
    "This section will provide a simple template for model-based reinforcement learning.\n",
    "\n",
    "The goal in model-based RL is to sample game iterations and thereby train a model which correctly predicts the future environment state and corresponding reward, given the current state and candidate action. \n",
    "\n",
    "The main advantage of model-based RL is that a model for the environment's evolution allows the agent to predict the cumulative reward over many future steps. This allows the agent to 'plan ahead', hopefully leading to a more successful policy.\n",
    "\n",
    "The choice of action can then be as simple as choosing the action which leads to the largest cumulative future reward.\n",
    "\n",
    "Below, we define a basic ModelBasedAgent class which can serve as a template for creating your own custom model-based agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent(Agent):\n",
    "    \"\"\"\n",
    "    Template for a model-based RL agent. \n",
    "    \n",
    "    To instantiate an agent, the user must provide the following functions:\n",
    "    \n",
    "    policy_function(): A method for choosing what action to take next. This can be a selection among a  discrete set\n",
    "                       of actions (as in the probability of each legal move for a current board position),\n",
    "                       or a function which chooses an action vector by some heuristic (applied when the action space \n",
    "                       is a continuous-valued vector with like the four torques for our Bipedal Walker).\n",
    "                       \n",
    " \n",
    "    \n",
    "    action_generator(): A method for generating candidate actions. This can be deterministic (generate *all* legal moves\n",
    "                        for a given chess board position) or probabilistic (generate candidate actions by sampling from\n",
    "                        some - possibly learned - distribution over the action space)\n",
    "                        \n",
    "    environment_model(): A method for predicting the next environment state given the current environment state and the \n",
    "                         candidate action. \n",
    "                         \n",
    "    reward_predictor(): A method for predicting the reward for a given state:action pair (s_t, a_t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_function, action_generator, environment_model, reward_predictor):\n",
    "        self.policy = policy_function\n",
    "        self.action_generator = action_generator\n",
    "        self.environment_model = environment_model        \n",
    "        self.reward_predictor = reward_predictor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_based_policy(current_state, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    The model-based agent chooses an action by iteratively generating candidate actions and the corresponding\n",
    "    predicted future environment state, up to some depth, and then predicting the reward for each\n",
    "    candidate action in each trajectory. \n",
    "    The agent then applies its policy function to select the action which is expected to maximize the future\n",
    "    cumulative reward. \n",
    "    \n",
    "    In this example the policy is simply to choose the action which maximizes the predicted cumulative reward\n",
    "    over the next <depth> number of steps. That is, the action which leads to the largest total reward across\n",
    "    all subsequent steps.\n",
    "    \n",
    "    This implementation generates the default number of candidate trajectories from the action_generator method\n",
    "    (say, <C> candidate trajectories) at each step, leading to <C>**<depth> random sample trajectories to compare.\n",
    "\n",
    "    Returns the next action to be input to the environment.\n",
    "    \"\"\"\n",
    "    action_generator = kwargs['action_generator']\n",
    "    environment_model = kwargs['environment_model']\n",
    "    reward_predictor = kwargs['reward_predictor']\n",
    "    depth = kwargs['depth']\n",
    "        \n",
    "    # Generate a list of randomly sampled game trajectories\n",
    "    sampled_trajectories = {\"0\": [current_state, 0, 0]}\n",
    "    for d in range(depth):\n",
    "        keys_to_expand = [k for k in sampled_trajectories.keys() if k[-1]==str(d)]\n",
    "\n",
    "        for key in keys_to_expand:\n",
    "            state = sampled_trajectories[key][0]\n",
    "            candidate_actions =  action_generator(state)\n",
    "            \n",
    "            for num, action in enumerate(candidate_actions):\n",
    "                new_state = environment_model(state, action)\n",
    "                reward = reward_predictor(current_state, action)\n",
    "                new_key = key + str(num)\n",
    "                sampled_trajectories.update({new_key: [new_state, action, reward]})\n",
    "                # Now add the reward from the candidate action to the parent node in the search tree\n",
    "                sampled_trajectories[key][2] += reward\n",
    "                \n",
    "    # Apply the policy function to the sampled trajectories. \n",
    "    # In this case, the policy is to choose the maximum cumulative predicted reward at depth 1.\n",
    "    action_reward_pairs = [(el[1], el[2]) for key, el in sampled_trajectories.items() if key[-1]==\"1\"]\n",
    "    best_action = max(action_reward_pairs, key=itemgetter(1))[0]\n",
    "\n",
    "    return best_action\n",
    "\n",
    "        \n",
    "def model_based_action_generator(state):\n",
    "    \"\"\"\n",
    "    The action generator for our model-free agent. In this example, the function generates four random action vectors.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=(4,4))\n",
    "\n",
    "\n",
    "def model_based_environment_model(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The environment model for our model-based agent. In this example, the function generates a random state vector.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return ENV.observation_space.sample()\n",
    "\n",
    "\n",
    "def model_based_reward_predictor(current_state, candidate_action):\n",
    "    \"\"\"\n",
    "    The function to predict the reward for a given environment state and candidate action.\n",
    "    \n",
    "    (This code is a template which should be replaced for optimal agent performance)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-100.0, high=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our template model-based agent. We expect it to do about as well as the random agent from above, since the candidate actions and predicted rewards are providing absolutely zero additional information to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.24223374316765"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_based_agent = ModelBasedAgent(model_based_policy, model_based_action_generator,\n",
    "                                       model_based_environment_model, model_based_reward_predictor)\n",
    "\n",
    "my_model_based_agent.play(ENV,\n",
    "                          steps=100,\n",
    "                          action_generator=my_model_based_agent.action_generator,\n",
    "                          environment_model=my_model_based_agent.environment_model,\n",
    "                          reward_predictor=my_model_based_agent.reward_predictor,\n",
    "                          depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we currently do not include any training for either model-based or model-free RL. This is still to come!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
