{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "This notebook serves as a simple working example of how to interact with OpenAI's AI Gym.\n",
    "\n",
    "## Environment Setup\n",
    "1. Install swig: https://www.dev2qa.com/how-to-install-swig-on-macos-linux-and-windows/\n",
    "2. Set up a python venv (optional):\n",
    "\n",
    "`pip3 install virtualenv`\n",
    "\n",
    "`python3 -m virtualenv venv`\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "3. Install required python packages:\n",
    "`pip3 install gym==0.17.2 box2d-py==2.3.8`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are used for defining the actions and states of the game \"BipedalWalker-v3\"\n",
    "\n",
    "BipedalWalker has 2 legs. Each leg has 2 joints. You can apply the torque on each of these joints in the range of (-1, 1)\n",
    "\n",
    "The state of the game is given by 24 variables, described in more detail here: https://github.com/openai/gym/wiki/BipedalWalker-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\duncan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "STATE_SPACE = 24\n",
    "ACTION_SPACE = 4\n",
    "ENV = gym.make(\"BipedalWalker-v3\")\n",
    "\n",
    "# actions\n",
    "Hip_1 = 0\n",
    "Knee_1 = 1\n",
    "Hip_2 = 2\n",
    "Knee_2 = 3\n",
    "\n",
    "# state\n",
    "HULL_ANGLE = 0\n",
    "HULL_ANGULAR_VELOCITY = 1\n",
    "VEL_X = 2\n",
    "VEL_Y = 3\n",
    "HIP_JOINT_1_ANGLE = 4\n",
    "HIP_JOINT_1_SPEED = 5\n",
    "KNEE_JOINT_1_ANGLE = 6\n",
    "KNEE_JOINT_1_SPEED = 7\n",
    "LEG_1_GROUND_CONTACT_FLAG = 8\n",
    "HIP_JOINT_2_ANGLE = 9\n",
    "HIP_JOINT_2_SPEED = 10\n",
    "KNEE_JOINT_2_ANGLE = 11\n",
    "KNEE_JOINT_2_SPEED = 12\n",
    "LEG_2_GROUND_CONTACT_FLAG = 13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the basic interface with the game. The game is essentially a very rapid turn-based game. In each round, there are 2 main steps:\n",
    "1. An action is taken by the agent.\n",
    "2. The game updates according to its current state and the input action from the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent, env, score_augmentation=False, show=False, verbose=False):\n",
    "    state = env.reset()\n",
    "    cumulative_score = 0\n",
    "    if show:\n",
    "        env.render()\n",
    "    for step in range(500):\n",
    "        action = agent(state)\n",
    "        state, reward, terminal, info = env.step(action)\n",
    "        if verbose:\n",
    "            print(state)\n",
    "        if show:\n",
    "            env.render()\n",
    "        cumulative_score += reward\n",
    "    env.close()\n",
    "    return cumulative_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets a positive reward proportional to the distance walked on the terrain. It can get a total of 300+ reward all the way up to the end.\n",
    "If agent tumbles, it gets a reward of -100.\n",
    "There is some negative reward proportional to the torque applied on the joint. So that agent learns to walk smoothly with minimal torque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define a couple of test agents to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(state):\n",
    "    \"\"\"This agent returns random actions.\"\"\"\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "\n",
    "\n",
    "def stupid_agent(state):\n",
    "    \"\"\"A very simple expert system.\"\"\"\n",
    "    if state[LEG_1_GROUND_CONTACT_FLAG] == 1 and state[LEG_2_GROUND_CONTACT_FLAG] == 1:\n",
    "        return np.random.uniform(low=-1.0, high=1.0, size=4)\n",
    "    if state[KNEE_JOINT_1_SPEED] < -0.5:\n",
    "        return np.array([0., 0.2, 0., 0.])\n",
    "    if state[KNEE_JOINT_2_SPEED] < -0.5:\n",
    "        return np.array([0., 0., 0., 0.2])\n",
    "    if abs(state[HULL_ANGLE]-pi) > pi/2:\n",
    "        return np.array([1., 1., 1., 1.])\n",
    "    return np.array([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-42314.801448657105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent = random_agent\n",
    "play_game(my_agent, ENV, show=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
